{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from kafka import TopicPartition\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "import json\n",
    "from botocore.vendored import requests\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark.sql \n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------------------------- SPARK CONTEXT -------------------------------\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "spark = SparkSession.builder.appName('CC_Fraud').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------- Load Model from File System -------------------------------\n",
    "\n",
    "def MachineLearningModelLoading():\n",
    "\tpath=\"/model1\"\n",
    "\tnewmodel=PipelineModel.load(path)\n",
    "\treturn newmodel\n",
    "\t\n",
    "newmodel = MachineLearningModelLoading()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexer_ad35a5fbb27e"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"merchant\", \"category\", \"gender\", \"job\"]\n",
    "indexers = []\n",
    "for i in range(len(cols)):\n",
    "    indexers.append(StringIndexer.load('C:/Users/saritajoshi/Documents/Repos/Credit-Card-Fraud-Detection-Spark/indexers/'+cols[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in df.columns if col != \"is_fraud\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'amt', 'category', 'cc_num', 'city', 'city_pop', 'dob', 'first', 'gender', 'is_fraud', 'job', 'last', 'lat', 'long', 'merch_lat', 'merch_long', 'merchant', 'state', 'street', 'trans_date_trans_time', 'trans_num', 'unix_time', 'zip']\n",
      "['_c0', 'amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'merchant', 'category', 'gender', 'job']\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------ Data Cleaning -------------------------------------\n",
    "def predicting_status(data_all,newmodel):\n",
    "\tcolumns_new = [col.replace(\"-\", \"_\") for col in data_all.columns]\n",
    "\tdata_all = data_all.toDF(*columns_new)\n",
    "\n",
    "\txvars = ['Unnamed: 0', 'merchant','category','amt','gender','lat','long','city_pop','job','unix_time','merch_lat','merch_long'] #all numeric\n",
    "\tselect_cols = xvars\n",
    "\tdata = data_all.select(select_cols)\n",
    "\tindexed_data = data\n",
    "\tfor indexer in indexers:\n",
    "\t\tindexed_data = indexer.fit(indexed_data).transform(indexed_data)\n",
    "\tindexed_data = indexed_data.drop(\"merchant\", \"category\", \"gender\", \"job\")\n",
    "\tindexed_data = indexed_data.withColumnRenamed(\"merchant_index\", \"merchant\").withColumnRenamed(\"category_index\", \"category\").withColumnRenamed(\"gender_index\", \"gender\").withColumnRenamed(\"job_index\", \"job\").withColumnRenamed('Unnamed: 0', '_c0')\n",
    "\t\n",
    "\tpred1 = newmodel.transform(indexed_data)\n",
    "\treturn pred1.select(\"prediction\")\n",
    "\n",
    "# df = spark.read.json(sc.parallelize([dd]))\n",
    "# res = predicting_status(df, newmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------- CASSANDRA CONNECTION -------------------------------------------------------\n",
    "cluster = Cluster()\n",
    "session = cluster.connect()\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS bigdata WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 3}\")\n",
    "session.set_keyspace(\"bigdata\")\n",
    "#session.execute(\"DROP TABLE testing\")\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS testing (txn_id varchar PRIMARY KEY,cc_no bigint ,txn_time varchar, year smallint, month smallint, day smallint, hour smallint, minute smallint,amount double, cc_provider int, merchant varchar,merchant_index smallint, country_name varchar, country_code varchar,country_index smallint, age int, marital_status varchar,marital_status_index smallint,gender varchar,gender_index smallint,loan varchar,loan_index smallint, status varchar,status_index smallint)\")\n",
    "# session.set_keyspace(\"mykeyspace\")\n",
    "# session.execute(\"CREATE TABLE IF NOT EXISTS training (txn_id varchar PRIMARY KEY,cc_no bigint , cc_provider int, year smallint, month smallint, day smallint, hour smallint, min smallint,txn_time varchar, merchant varchar, location varchar, country varchar, amount float, status varchar)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'transaction_data'\n",
    "consumer = KafkaConsumer(topic_name, group_id='test-producer', bootstrap_servers=['localhost:9092'])\n",
    "dd= {}\n",
    "for msg in consumer:\n",
    "\tmsg=msg.value.decode(\"utf-8\")\n",
    "\tdf = spark.read.json(sc.parallelize([msg]))\n",
    "\tstatus_predicted=predicting_status(df,newmodel)\n",
    "\tprint(msg)\n",
    "\tif status_predicted.collect()[0][0]==0:\n",
    "\t\tstatus=\"Not Fraud\"\n",
    "\telse:\n",
    "\t\tstatus=\"Fraud\"\n",
    "\tmsg['status'] = status\n",
    "\tprint(msg)\n",
    "\n",
    "\tstrq=\" INSERT INTO testing JSON \"+\"'\"+json.dumps(msg)+\"'\"\n",
    "\tsession.execute(strq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
